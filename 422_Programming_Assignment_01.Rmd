---
title: '422-57: Programming Assignment 01'
author: 'Michael Gilbert'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  pdf_document: 
    highlight: tango
    fig_height: 5
    fig_width: 6
---

\

Workspace cleanup and prep:
```{r setup.R, message = F, warning = F}
# Clear workspace
rm(list=ls())

# Load packages
library(fitdistrplus)
library(knitr)
library(RCurl)
```

```{r setup.knitr, include = F}
# Set code width to 60 to contain within PDF margins
knitr::opts_chunk$set(tidy = F, tidy.opts = list(width.cutoff = 60))

# Set all figures to be centered
knitr::opts_chunk$set(fig.align = "center")

# Set and perserve par(mfcol()) between chunks (calls to it can be hidden)
knitr::opts_knit$set(global.par = T)
```

## ISLR, Section 2.4
### Exercise 8:

This exercise relates to the `College` data set, which can be found in the file `College.csv`. It contains a number of variables for the 777 different universities and colleges in the US. _[Note: variable list excluded]_

(a) Use the `read.csv()` function to read the data into `R`. Call the loaded data `college`. Make sure that you have the directory set to the correct location for the data.
\
```{r Ex8a, indent = "    "}
# Download and assign data
if(!file.exists("~/College.csv")){
    URL <- getURL("http://www-bcf.usc.edu/~gareth/ISL/College.csv")
    college <- read.csv(textConnection(URL), header = T)
    rm(URL)
}
```
    
(b) Look at the data using the `fix()` function. You should notice that the  first column is just the name of each university. We don't really want `R` to treat this as data. However, it may be handy to have these names for later. Try the following commands:
    
```{r Ex8b1a, indent = "    ", eval = F}
> rownames(college)=college[,1]
> fix(college)
```
\
```{r Ex8b1b, indent = "    "}
# Execute commands
rownames(college) <- college[, 1]
fix(college)
```
    
    You should see that there is now a `row.names` column with the name of each  university recorded. This means that `R` has given each row a name corresponding to the appropriate university. `R` will not try to perform calculations on the  row names. However, we still need to eliminate the first column in the data  where the names are stored. Try:
    
```{r Ex8b2a, indent = "    ", eval = F}
> college=college[,-1]
> fix(college)
```
\
```{r Ex8b2b, indent = "    "}
# Execute commands
college <- college[, -1]
fix(college)
```
    
    Now you should see that the first data column is `Private`. Note that another column labeled `row.names` now appears before the `Private` column. However, this is not a data column but rather the name that `R` is giving to each row.

(c) 
    i. Use the `summary()` function to produce a numerical summary of the variables in the data set.
\
```{r Ex8c1, indent = "        "}
# Explore summary stats
summary(college)
```
    
    ii. Use the `pairs()` function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix `A` using `A[,1:10]`.
\
```{r Ex8c2, indent = "        ", fig.width = 8, fig.height = 7}
# Scatterplot matrix of first ten columns
pairs(college[, 1:10], 
      main = "Scatterplot Matrix: First 10 Columns of 'College.csv'")
```
    
    iii. Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private`.
\
```{r Ex8c3, indent = "        ", fig.width = 7, fig.height = 6}
# Boxplots of Outstate versus Private
boxplot(college$Outstate ~ college$Private,
        col = "beige", main = "Out-of-State Tuition versus Private", 
        ylab = "Tuition Cost per Student in $")
```
    
    iv. Create a new qualitative variable, called `Elite`, by _binning_ the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.
    
```{r Ex8c4a, indent = "        ", eval = F}
> Elite=rep("No",nrow(college))
> Elite[college$Top10perc >50]="Yes"
> Elite=as.factor(Elite)
> college=data.frame(college, Elite)
```
\
```{r Ex8c4b, indent = "        "}
# Execute commands
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
rm(Elite)
```
    
        Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite`.
    
```{r Ex8c4c, indent = "        "}
# Use summary() to determine count of elite universities
summary(college$Elite)
```
    
```{r Ex8c4d, indent = "        ", fig.width = 7, fig.height = 6}
# Boxplots of Outstate versus Elite
boxplot(college$Outstate ~ college$Elite,
        col = "beige", main = "Out-of-State Tuition versus Elite", 
        ylab = "Tuition Cost per Student in $")
```
    
    v. Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow=c(2,2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.
\
        _Note: Histograms in this question and the question that follows are plotted with bins of equally spaced probabilities. The widths of the bins vary, but the area represented by each bin is the same. The varying widths can help illustrate properties of the distribution._
    
```{r Ex8c5a, include = F}
par(mfcol = c(2, 2))
```
    
```{r Ex8c5b, indent = "        ", fig.width = 8, fig.height = 7}
# Identify variable classes
sapply(college, class)

#----------------------------------------------------------------------
# Create new variables as necessary and merge with college
#----------------------------------------------------------------------

# Accept.Rate
Accept.Rate <- round((college$Accept / college$Apps), 4)
college <- data.frame(college, Accept.Rate)
rm(Accept.Rate)

# Enroll.Rate
Enroll.Rate <- round((college$Enroll / college$Accept), 4)
college <- data.frame(college, Enroll.Rate)
rm(Enroll.Rate)

#----------------------------------------------------------------------
# Plot histograms
#----------------------------------------------------------------------

# Set sequence for deciles
seq <- seq(0.0, 1.0, by = 0.1)

# Plots
hist(college$Accept.Rate, col = "beige",
     breaks = quantile(college$Accept.Rate, probs = seq))
hist(college$Grad.Rate, col = "beige", 
     breaks = quantile(college$Grad.Rate, probs = seq))
hist(college$Enroll.Rate, col = "beige",
     breaks = quantile(college$Enroll.Rate, probs = seq))
hist(college$Top10perc, col = "beige",
     breaks = quantile(college$Top10perc, probs = seq))
```
    
```{r Ex8c5c, include = F}
par(mfcol = c(1, 1))
```
    
    vi. Continue exploring the data, and provide a brief summary of what you discover.
\
        An earlier question looked at the basic statistics of variables from the `summary()` function in `{base}`. Two interesting variables are explored further here.
\
\
        __Variable: Grad.Rate__
\
        _where, Grad.Rate = graduation rate_
    
```{r Ex8c6a, indent = "        ", fig.width = 8, fig.height = 7, warning = F}
# Check fit of distribution
plot(fitdist(college$Grad.Rate, "norm", method = "mle"), demp = T,
     breaks = quantile(college$Grad.Rate, probs = seq(0.0, 1.0,
     by = 0.1)))
```
    
        `Grad.Rate` appears to fit the theoretical normal distribution well. The Q-Q plot shows deviation in the upper tail, with one extreme observation. This observation is interesting as it could be said to exert _leverage_ but not _influence_ since it is distanced from the majority of observations, yet still appears 'in-line' with those observations and falls on the theoretical normal distribution line. The 'plateau' seen in the upper tail of the Q-Q plot is from the empirical quantile values not exceeding 100, which makes sense for the graduation rate itself, but perhaps not for the _frequency_ with which the observations occur.
        
        The empirical quantile value of this extreme observation is peculiar: it is greater than 100. A coffee mug cannot hold more than 100 percent of its volume. _Prima facie_,  it seems a post-secondary institution cannot have a graduation rate above 100 percent. The gut response is this value was entered erroneously, perhaps due to an actor hitting the number `1` twice. But, this also presents interesting philosophical questions related to data models and business processes: 
        
        * What if this institution distributed an outsized number of honorary degrees, and those were counted in the computing `Grad.Rate`? 
        * For an institution to distribute an honorary degree, must it first _accept_ the recipient as a student? 
        * If so, what effect does that have on financial transactions (e.g. a student must apply to be accepted, to apply requires payment of an application fee)?
        
\
        
        `Grad.Rate` was sorted and examined further:
    
```{r Ex8c6b, indent = "        "}
# Sort by Grad.Rate
college <- college[order(college$Grad.Rate), ]

# Examine the last 15 observations
tail(data.frame(college = row.names(college), college$Grad.Rate), n = 15)
```
    
        The results pass the 'smell test': it does not _seem_ unreasonable for the institutions listed to have such high observation values for `Grad.Rate` (with the exception of Cazenovia College).
\
\
        __Variable: Top10perc__
\
        _where, Top10perc = new students from top 10 percent of high school class_
    
```{r Ex8c6c, indent = "        ", fig.width = 8, fig.height = 7, warning = F}
# Check fit of distribution
plot(fitdist(college$Top10perc, "weibull", method = "mle"), demp = T,
     breaks = quantile(college$Top10perc, probs = seq(0.0, 1.0,
     by = 0.1)))
```
    
        While other variables were also explored, `Top10perc` is included here because it appears to have a decent fit to the theoretical Weibull distribution. Frankly, it was pretty cool to see a distribution other than the more common ones explored so far in the MSPA program (e.g. Normal, Student's t, Poisson, Binomial, Negative Binomial, etc.).
        
        The Q-Q plot shows deviation in the upper tail. Similar to `Grad.Rate`, the empirical quantile values beg the question of _realism_. `Top10perc` was sorted and examined further:
    
```{r, Ex8c6d, indent = "        "}
# Sort by Top10perc
college <- college[order(college$Top10perc), ]

# Examine the last 15 observations
tail(data.frame(college = row.names(college), college$Top10perc), n = 15)
```
    
        The results pass the 'smell test': it does not _seem_ unreasonable for the institutions listed to have such high observation values for `Top10perc`.
        
### Exercise 9:

This exercise involves the `Auto` data set studied in the lab. Make sure that the missing values have been removed from the data.
\
```{r Ex9base1}
# Download and assign data
if(!file.exists("~/Auto.csv")){
    URL <- getURL("http://www-bcf.usc.edu/~gareth/ISL/Auto.csv")
    auto <- read.csv(textConnection(URL), header = T)
    rm(URL)
}

# Examine dimensions (pre-NA removal)
dim(auto)

# Assign new data.frame with missing values removed
auto <- na.omit(auto)

# Examine dimensions (post-NA removal)
dim(auto)
```

Since the dimensions pre- and post-NA removal are identical, there _must_ not be any missing values, right? _Not so!_ Perhaps `R` failed to correctly identify `NA` values (or characters) during the import. If those were known before importing, they could have been handled with `read.csv(na.strings = "X")`, where `X` is the character corresponding to the `NA` value.

A more detailed look at possible `NA` values follows. Starting with the `class()` function:

```{r Ex9base2}
# Examine variable classes
sapply(auto, class)
```

Notice anything odd? `horsepower` and `name` are listed as having class `factor`. However, in `R`, categorical variables (such as the make and model of a motor vehicle) can be treated as class `factor` (see `?factor` for more information), so `name` _likely_ does not contain missing values.

Further examination of `horsepower` shows some observations contain a `?` character. Those should be treated as `NA` values and removed:

```{r Ex9base3}
# Sort by horsepower
auto <- auto[order(auto$horsepower), ]

# Examine the first 10 observations
head(data.frame(auto = row.names(auto), auto$horsepower), n = 10)

# Examine dimensions (pre-NA removal)
dim(auto)

# Treat "?" as NA
auto[auto == "?"] <- NA

# Assign new data.frame with missing values removed
auto <- na.omit(auto)

# Examine dimensions (post-NA removal)
dim(auto)
```

Viola!

(a) Which of the predictors are quantitative, and which are qualitative?
\
    Both quantitative and qualitative data may be further divided:
        
    * _Quantitative_ data may be _interval-level_ data or _ratio-level_ data. These data types are also referred to as _continuous_ data.
        
    * _Qualitative_ data may be _nominal-level_ data or _ordinal-level_ data. These data types are also referred to as _categorical_ data.
    
```{r Ex9a1, eval = F, include = F}
# Another way to remember this: 
# Quantitative variables ask "How many?" (discrete) or "How much?" (continuous)
# Qualitative variables ask "Which?"
# Source: http://pirate.shu.edu/~hovancjo/exp/classification_of_variables.htm
```
    
    Each of the variables of the `Auto` data are examined below. A brief justification accompanies the reasoning behind the classification.
    
    * `mpg` refers to the miles per gallon a motor vehicle is estimated to get, and is a _quantitative_ predictor. The values [should all] be numeric, and the ratio between values has meaning. A motor vehicle with `mpg` of 20 gets half as many miles per gallon as a motor vehicle with `mpg` of 40. `mpg` is also a _continuous_ variable as it can take on any possible value.
    
    * `cylinders` refers to the number of cylinders a motor vehicle has, and is a _quantitative_ predictor. `cylinders` is also a _discrete_ variable as it can only take on integer values. While the number of cylinders may colloquially be referred to as a "category", the ratio between values does have meaning. However, _depending on the purpose of the model and the task at hand_, `cylinders` could also be treated as a _qualitative_ predictor, since the range of values is relatively small, and using `cylinders` as a classifier could be interesting.
    
    * `displacement` is a measure of volume in engine cylinders, and is a _quantitative_ predictor. `displacement` is also a _continuous_ variable as it can take on any possible value (within upper and lower bounds).
    
    * `horsepower` is a measure of the amount of work an engine can produce, and is a _quantitative_ predictor. `horsepower` is also a _continuous_ variable as it can take on any possible value.
    
    * `weight` is a measure of gravitational pull, and is a _quantitative_ predictor (while your `weight` may vary between here and Mars, your `mass` will not). `weight` is also a _continuous_ variables as it can take on any possible value.
    
    * `acceleration` is a measure of distance covered over time (often expressed in the amount of time to go from one velocity to another, e.g. 0-60 mph), and is a _quantitative_ predictor. `acceleration` is also a _continuous_ variable as it can take on any possible value.
    
    * `year` is a measure of time, specifically the model year of the motor vehicle, and is a _quantitative_ predictor. In the `Auto` data set, `year` is also a _discrete_ variable as it can only take on integer values. There are rare instances of a motor vehicle being assigned a half-year value, though even then the designation is often not one from the manufacturer (e.g. 1964.5 Ford Mustang).
    
    * `origin` takes on a numeric value, however is a _qualitative_ predictor at heart as each value corresponds to a continent or country the motor vehicle was produced in (or the headquarters of the brand). An `origin` of `1` corresponds to the United States, of `2` to Europe, and of `3` to Japan.
    
    * `name` refers to the make and model of the motor vehicle, and is a _qualitative_ predictor. Without _a priori_ knowledge, it is impossible to judge or make comparisons between observations with different values for `name`.
    
    The variable `origin` was converted to a factor variable, a factor version of the `cylinders` variable was created and merged with the `Auto` data, and `horsepower` was converted to a numeric variable:
    
```{r Ex9a2, indent = "    "}
# Convert origin to factor
auto$origin <- as.factor(auto$origin)

# Create factor version of cylinder and merge
cylinders.fac <- as.factor(auto$cylinder)
auto <- data.frame(auto, cylinders.fac)
rm(cylinders.fac)

# Rename integer version of cylinder
auto$cylinders.int <- auto$cylinders

# Drop old version of cylinder
auto <- subset(auto, select = -cylinders)

# Convert horsepower to numeric
# Note: when converting factors to numeric, first convert to character;
#   this preserves any decimals present in data
auto$horsepower <- as.numeric(as.character(auto$horsepower))
```
    
(b) What is the _range_ of each quantitative predictor? You can answer this using the `range()` function.
\
```{r Ex9b, indent = "    ", results = "hold"}
# Assign temp data.frame() IFF variables are not factors
temp <- auto[, !sapply(auto, is.factor)]

# Use apply, but must transpose as it produces a 2x7 matrix
temp <- t(apply(temp, 2, function(x) range(x)))

# Add column names
colnames(temp) <- c("Min", "Max")

# Round to two digits, then remove temp
round(temp, digits = 2); rm(temp)
```
    
(c) What is the mean and standard deviation of each quantitative predictor?
\
```{r Ex9c, indent = "    ", results = "hold"}
# Assign temp data.frame() IFF variables are not factors
temp <- auto[, !sapply(auto, is.factor)]

# Use apply, but must transpose as it produces a 2x7 matrix
temp <- t(apply(temp, 2, function(x) c(mean(x), sd(x))))

# Add column names
colnames(temp) <- c("Mean", "Std. Deviation")

# Round to two digits, then remove temp
round(temp, digits = 2); rm(temp)
```
    
(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
\
    From the beginning of Exercise 9, the `Auto` data set was sorted on `horsepower` to easily identify the `NA` character `?`. In order to remove the intended observations, the `Auto` data set should be sorted by `row.names`, which serve as an index key in this instance.
    
```{r Ex9d1, indent = "    "}
# Return to original sort from import
auto <- auto[order(as.numeric(row.names(auto))), ]

# Remove 10th through 85th observation
auto.rm <- auto[-c(10:85), ]
```
    
    Now continue:
    
```{r Ex9d2, indent = "    ", results = "hold"}
# Assign temp data.frame() IFF variables are not factors
temp <- auto.rm[, !sapply(auto.rm, is.factor)]

# Use apply, but must transpose as it produces a 4x7 matrix
temp <- t(apply(temp, 2, function(x) c(range(x), mean(x), sd(x))))

# Add column names
colnames(temp) <- c("Min", "Max", "Mean", "Std. Deviation")

# Round to two digits, then remove temp
round(temp, digits = 2); rm(temp)
```
    
(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.
\
    First, a `data.frame` called `temp` was created of non-factor variables.
    
```{r Ex9e1, indent = "    ", fig.width = 8, fig.height = 7}
# Assign temp data.frame() IFF variables are not factors
temp <- auto[, !sapply(auto, is.factor)]
```    
    
    Next, a scatterplot matrix was created of the non-factor variables. Factor variables use _levels_ and a scatterplot is not very useful for such variables.

```{r Ex9e2, indent = "    ", fig.width = 8, fig.height = 7}
# Scatterplot matrix of non-factor variables
pairs(temp, main = "Scatterplot Matrix: Non-factor Variables of 'Auto.csv'")
```
    
    Following the scatterplot matrix, histograms and boxplots were created for each non-factor variable. Similar to scatterplots, visual exploration of factor variables may not be very useful with these plots. For instance, see both plots of the `auto$cylinders.int` variable. Earlier, this was identified as a factor variable, but a second version of the variable as a numeric variable was created (the variable could be _qualitative_ or _quantitative_ depending on the purpose of the model and how the variable is used).
    
```{r include = F}
par(mfcol = c(2, 2))
```

```{r Ex9e3, indent = "    ", fig.width = 8, fig.height = 7}
# Create histograms
for (i in 1:ncol(temp)) {
    hist(temp[, i], col = "beige", 
         main = paste("Histogram of auto$", names(temp)[i], sep = ""),
         xlab = paste("auto$", names(temp)[i], sep = ""))
}
```

```{r include = F}
par(mfcol = c(1, 1))
```

```{r include = F}
par(mfcol = c(2, 2))
```

```{r Ex9e4, indent = "    ", fig.width = 8, fig.height = 7}
# Create boxplots
for (i in 1:ncol(temp)) {
    boxplot(temp[, i], col = "beige", 
            main = paste("Boxplot of auto$", names(temp)[i], sep = ""),
            ylab = paste("auto$", names(temp)[i], sep = ""))
}
```

```{r include = F}
par(mfcol = c(1, 1))
```
    
    Finally, the `data.frame(temp)` was removed to keep the workspace tidy.
    
```{r Ex9e5, indent = "    "}
# Remove temp
rm(temp)
```
    
    __Findings__: Despite `year` and `cylinders.int` being _numeric_ variables, there is no clear linear relationship against other variables. That said, `clyinders.int` might be used to create _flag_, _dummy_, or _indicator_ variables (here, these terms are used interchangeably). How do these relationships change when they are classified by levels of `cylinders.int`? With `year`, it is clear that as time goes on the general `mpg` improves. Perhaps classifying `name` into something like `vehicle.type` would be a useful relationship to explore. 
    
(f) Suppose that we wish to predict gas mileage (`mpg`) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting `mpg`? Justify your answer.
\
    In addition to the plots, the correlation between `mpg` and other non-factor variables was examined.
    
```{r Ex9f1, indent = "    "}
# Examine correlation between scatterplot variables
sapply(auto[, !sapply(auto, is.factor)], function(x) cor(auto$mpg, x))
```
    
    `mpg` has a correlation of `1.0` since it is perfectly correlated with itself. Correlation values can range from `-1.0` to `1.0`. The variables `weight`, `displacement`, `horsepower`, and `cylinders.int` all show strong negative correlation with `mpg`. These findings are intuitive and also match those of the scatterplot matrix in the preceding question.
    
```{r FIN, eval = F}
# FIN
```

```{r ALT, include = F, eval = F}
# Alternate approach for Ex9b
temp <- NULL
for (i in 1:ncol(auto)) {
    if(is.factor(auto[, i]) == F) {
        temp <- rbind(temp, data.frame(colnames(auto[i]), 
                                       round(min(auto[, i]), digits = 2),
                                       round(max(auto[, i]), digits = 2)))
    }
}
colnames(temp) <- c("Variable", "Min", "Max")
temp
rm(temp)

# Alternate approach for Ex9c
temp <- NULL
for (i in 1:ncol(auto)) {
    if(is.factor(auto[, i]) == F) {
        temp <- rbind(temp, data.frame(colnames(auto[i]),
                                       round(mean(auto[, i]), digits = 2),
                                       round(sd(auto[, i]), digits = 2)))
    }
}
colnames(temp) <- c("Variable", "Mean", "Standard Deviation")
temp
rm(temp)

# Alternate approach for Ex9d
auto.rm <- auto[-c(10:85), ]
temp <- NULL
for (i in 1:ncol(auto)) {
    if(is.factor(auto[, i]) == F) {
        temp <- rbind(temp, data.frame(colnames(auto.rm[i]),
                                       round(min(auto.rm[, i]), digits = 2),
                                       round(max(auto.rm[, i]), digits = 2),
                                       round(mean(auto.rm[, i]), digits = 2),
                                       round(sd(auto.rm[, i]), digits = 2)))
    }
}
colnames(temp) <- c("Variable", "Min", "Max", "Mean", "Standard Deviation")
temp
rm(temp)
```
